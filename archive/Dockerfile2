# Use an Airflow base image
FROM apache/airflow:2.6.3

# Set build arguments for Spark
ARG SPARK_VERSION=3.2.0
ARG HADOOP_VERSION=3.2

# Install Java and dependencies
USER root
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        openjdk-11-jdk \
        wget \
        curl \
        ca-certificates \
        gnupg \
        libpq-dev \
        software-properties-common \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Spark
RUN curl -fSL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -o /tmp/spark.tgz \
    && tar -xzf /tmp/spark.tgz -C /opt/ \
    && mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark \
    && rm /tmp/spark.tgz

# Set environment variables for Java and Spark
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV SPARK_HOME=/opt/spark
ENV PATH="$PATH:$JAVA_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin"

# Install Python packages required for Airflow DAGs
USER airflow
RUN pip install --no-cache-dir \
    nba_api \
    pandas \
    confluent-kafka \
    psycopg2-binary \
    paramiko \
    pyspark \
    kafka-python

# Switch back to root to ensure proper permissions
USER root

# Ensure Spark and Java paths exist and adjust ownership if needed
RUN test -d $SPARK_HOME && chown -R airflow: $SPARK_HOME || echo "$SPARK_HOME does not exist, skipping chown" \
    && test -d $JAVA_HOME && chown -R airflow: $JAVA_HOME || echo "$JAVA_HOME does not exist, skipping chown"

# Add Spark configuration directory
RUN mkdir -p $SPARK_HOME/conf \
    && chown -R airflow: $SPARK_HOME/conf

# Switch back to airflow user
USER airflow

# Copy Airflow configuration files, DAGs, and plugins if necessary
COPY dags /opt/airflow/dags
COPY plugins /opt/airflow/plugins

# Set the entrypoint to use Airflow commands by default
ENTRYPOINT ["/entrypoint"]
